import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd

# -----------------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ù„ÛŒ Ø§Ù¾
# -----------------------------
st.set_page_config(page_title="HTML Tree Viewer", layout="wide")

st.title("ğŸŒ HTML Tag Tree Viewer")
url = st.text_input("Enter URL:")

# -----------------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø±Ø§Ú©Ø³ÛŒ (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²)
# -----------------------------
USE_PROXY = True  # Ø§Ú¯Ø± Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ù¾Ø±Ø§Ú©Ø³ÛŒ ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ù‡ Ø¨Ø°Ø§Ø± False
PROXY = {
    "http": "http://46.209.15.187:8080",
    "https": "http://46.209.15.187:8080",
}

# -----------------------------
# ØªØ§Ø¨Ø¹ Ú¯Ø±ÙØªÙ† HTML Ø¨Ø§ Ù‡Ù†Ø¯Ù„ Ø®Ø·Ø§ Ùˆ Ù¾Ø±Ø§Ú©Ø³ÛŒ
# -----------------------------
def fetch_html(url):
    try:
        if USE_PROXY:
            response = requests.get(url, timeout=10, proxies=PROXY)
        else:
            response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.exceptions.ProxyError:
        st.warning("âš ï¸ Proxy failed â€” retrying without proxy...")
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            st.error(f"âŒ Failed to fetch {url}: {e}")
            return None
    except Exception as e:
        st.error(f"âŒ Error fetching {url}: {e}")
        return None

# -----------------------------
# ØªØ§Ø¨Ø¹ Ø³Ø§Ø®Øª Ø¯Ø±Ø®Øª HTML
# -----------------------------
def build_tree(element, level=0):
    indent = "  " * level
    result = f"{indent}<{element.name}>\n"
    for child in element.children:
        if child.name:
            result += build_tree(child, level + 1)
    return result

# -----------------------------
# Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ùˆ Ù†Ù…Ø§ÛŒØ´
# -----------------------------
if url:
    html_content = fetch_html(url)

    if html_content:
        soup = BeautifulSoup(html_content, "html.parser")
        body = soup.body

        if body:
            tree_text = build_tree(body)

            st.subheader("HTML Tag Tree")
            st.code(tree_text, language="html")

            st.subheader("Table of Tags")
            tags = [tag.name for tag in soup.find_all()]
            df = pd.DataFrame({"Tag": tags})
            st.dataframe(df)
        else:
            st.warning("No <body> tag found in this page.")
