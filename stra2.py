import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµÙØ­Ù‡
st.set_page_config(page_title="Web Scraper", page_icon="ğŸŒ", layout="wide")

# CSS Ø¨Ø±Ø§ÛŒ Ø²ÛŒØ¨Ø§ÛŒÛŒ
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .success-box {
        padding: 1rem;
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .error-box {
        padding: 1rem;
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸŒ Web Scraper Pro</div>', unsafe_allow_html=True)

def get_user_agent():
    """Ù„ÛŒØ³Øª User-Agent Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'
    ]
    return random.choice(user_agents)

def safe_request(url, max_retries=3):
    """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ù…Ù† Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÚ©Ø±Ø§Ø±"""
    headers = {
        'User-Agent': get_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    for attempt in range(max_retries):
        try:
            # ØªØ£Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
            if attempt > 0:
                delay = random.uniform(2, 5)
                time.sleep(delay)
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´Ø¯Ù‡
            if response.status_code == 200 and len(response.content) > 1000:
                return response
            else:
                st.warning(f"Attempt {attempt + 1}: Content too short, retrying...")
                
        except requests.exceptions.Timeout:
            st.warning(f"Attempt {attempt + 1}: Timeout occurred, retrying...")
        except requests.exceptions.HTTPError as e:
            st.error(f"Attempt {attempt + 1}: HTTP Error {e}")
            return None
        except requests.exceptions.RequestException as e:
            st.warning(f"Attempt {attempt + 1}: Connection error: {e}")
    
    st.error(f"Failed to fetch URL after {max_retries} attempts")
    return None

def extract_article_data(html_content):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ HTML"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù†
    title = soup.find('title')
    title_text = title.get_text().strip() if title else "Title not found"
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ù‚Ø§Ù„Ù‡ (Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø³Ø§Ø®ØªØ§Ø± setare.com)
    content_selectors = [
        '.article-content',
        '.content',
        '.entry-content',
        'article',
        '.news-content',
        '.story__content'
    ]
    
    content = None
    for selector in content_selectors:
        content = soup.select_one(selector)
        if content:
            break
    
    # Ø§Ú¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø§ØµÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² body Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if not content:
        content = soup.find('body')
    
    content_text = content.get_text().strip() if content else "Content not found"
    
    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†
    content_text = ' '.join(content_text.split()[:500])  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ø§Ø±Ø§Ú©ØªØ±
    
    return {
        'title': title_text,
        'content': content_text,
        'success': True
    }

def main():
    # sidebar Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    with st.sidebar:
        st.header("âš™ï¸ Settings")
        max_retries = st.slider("Max Retries", 1, 5, 3)
        timeout = st.slider("Timeout (seconds)", 10, 30, 15)
        
        st.header("â„¹ï¸ About")
        st.info("This app extracts article data from websites with anti-blocking features.")
    
    # Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ Enter URL")
        url = st.text_input(
            "Website URL",
            value="https://setare.com/fa/news/754575/",
            placeholder="https://example.com/article"
        )
        
        if st.button("ğŸš€ Extract Article Data", type="primary"):
            if url:
                with st.spinner("ğŸ”„ Fetching data..."):
                    response = safe_request(url, max_retries)
                    
                    if response and response.status_code == 200:
                        data = extract_article_data(response.content)
                        
                        if data['success']:
                            st.markdown('<div class="success-box">âœ… Data extracted successfully!</div>', unsafe_allow_html=True)
                            
                            st.subheader("ğŸ“° Article Title")
                            st.write(data['title'])
                            
                            st.subheader("ğŸ“– Article Content")
                            st.write(data['content'])
                            
                            # Ø¯Ú©Ù…Ù‡ Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù†
                            if st.button("ğŸ“‹ Copy Content"):
                                st.code(data['content'], language='text')
                                st.success("Content ready to copy!")
                        else:
                            st.markdown('<div class="error-box">âŒ Failed to extract article data</div>', unsafe_allow_html=True)
                    else:
                        st.markdown('<div class="error-box">âŒ Failed to fetch URL</div>', unsafe_allow_html=True)
            else:
                st.warning("âš ï¸ Please enter a URL")
    
    with col2:
        st.subheader("ğŸ’¡ Tips")
        st.info("""
        - Use reputable news websites
        - Avoid sites with heavy anti-bot protection
        - Some sites may block automated requests
        - Try different URLs if one fails
        """)
        
        st.subheader("âœ… Supported Sites")
        st.write("""
        - Setare.com
        - Most news websites
        - Blogs and articles
        """)

if __name__ == "__main__":
    main()
